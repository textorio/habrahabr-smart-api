//fullscreen

0:12 — первый слайд, 10 причин [не] использовать k8s.

Меня зовут Глушков Иван, я это будет рассказ, в котором я буду как защищать Kubernetes, так и ругать его. Поэтому я назвал его <b>[не]</b>.

//cut

//right

0:31 — второй слайд, Немного о себе.

Последние два места работы у меня так или иначе были связаны с Kubernetes. Я и в Postmates, и в Machine Zone работал в инфракомандах, и Kubernetes мы затрагиваем очень плотно. Плюс, я веду подкаст <a href="https://devzen.ru">DevZen</a>.

0:49 — третий слайд, План на сегодня.

Мы сегодня довольно плотно идем по времени, очень много слайдов, поэтому давайте вопросы в самом конце, если они будут. Сперва я пробегусь вкратце по области, почему это полезно и важно для многих, почему этот хайп возникает. Потом расскажу про наш опыт использования технологии. Ну и потом выводы.

1:15 — четвертый слайд, Слайды можно скачать.

Слайды можно скачать прямо сейчас, там будут примеры с конфигами. Особенно если вы не говорите по-русски, там они на английском.

1:28 — пятый слайд, Сделай свой выбор.

Я не буду строго всем говорить: обязательно используйте Kubernetes. Есть и плюсы, и минусы, поэтому если вы пришли сюда искать минусы, вы их найдете. Перед вами выбор, смотреть только на плюсы, только на минусы или в целом смотреть на все вместе. Плюсы мне будет помогать показывать Simon Cat, и черная кошка будет перебегать дорогу, когда есть минус.

1:56 — шестой слайд, Почему это вообще важно.

Итак, почему вообще произошел этот хайп, почему технология Х лучше, чем Y. Kubernetes — это точно такая же система, и существует их гораздо больше, чем одна. Есть Puppet, Chef, Ansible, Bash+SSH, Terraform. Мой любимый SSH помогает мне сейчас, зачем мне переходить куда-то. Я считаю, что критериев много, но я выделил самые важные.

Время от коммита до релиза — очень хорошая оценка, и ребята из Express 42 — большие эксперты в этом. Автоматизация сборки, автоматизация всего pipeline — это очень хорошая вещь, и она не может быть перехвалена, она на самом деле помогает. Continuous Integration, Continuous Deployment. И, конечно же, сколько усилий вы потратите на то, чтобы все сделать. Все можно написать на Ассемблере, как я говорю, систему деплоймента тоже, но удобства это не добавит.

//fullscreen_dirty

3:00 — седьмой слайд, Очень быстрое погружение в k8s

//right

Вкратце про Kubernetes я рассказывать не буду, вы знаете, что это такое. Я немного буду касаться этих областей дальше.

3:07 — восьмой слайд, Важность для разработчиков.

Почему это важно для разработчиков? Для них важна повторяемость, то есть, если они написали какое-то приложение, запустили тест, оно будет работать и у вас, и у соседа, и в продакшене. Второе — стандартизированное окружение: если вы изучили Kubernetes, пойдете соседнюю компанию, где есть Kubernetes, там будет все то же самое. Упрощение процедуры тестирования, Continuous Integration — это все не прямое следствие использования Kubernetes. но все равно это упрощает задачу, поэтому все становится удобнее.

//fullscreen_dirty

3:37 — девятый слайд, Важность для релиз-инженеров

Для релиз-разработчиков гораздо больше плюсов. Во-первых, это иммутабельная инфраструктура. Во-вторых, инфраструктура как код, который где-то хранится. В третьих, идемпотентность, возможность добавить релиз одной кнопкой. Откаты релизов происходят достаточно быстро, и интроспекция системы достаточно удобная. Конечно, все это можно сделать и в вашей системе, написанной на коленке, но вы не всегда можете это сделать правильно, а в Kubernetes это уже реализовано.

//left

4:11 — десятый слайд, Заблуждения

Чем Kubernetes не является, и что он не позволяет делать? Есть много заблуждений на этот счет. Начнем с контейнеров. Kubernetes работает поверх них. Контейнеры — не легковесные виртуальные машины, а совсем другая сущность. Их легко объяснить с помощью этого понятия, но на самом деле это неправильно. Концепция совершенно другая, надо понять и принять.

Во-вторых, Kubernetes не делает приложение более защищенным. Он не делает их автоматически скалируемым. Нужно сильно постараться, чтобы его запустить, не так, что нажал кнопку, и все автоматически заработало. Будет больно.

4:53 11 слайд, опыт

Наш опыт. Вот тут кот Саймона немножечко все поломал, а мы хотим, чтобы вы и все остальные ничего не ломали. Для этого нужно больше смотреть по сторонам — вот наша сторона.

5:06 12 слайд, Начало

Во-первых, Kubernetes не ходит в одиночку. Когда вы строите структуру, которая будет полностью управлять релизами и деплоями, вы должны понимать, что Kubernetes — один кубик, а таких кубиков должно быть 100. Чтобы все это построить, нужно все это сильно изучить. Новички, которые будут приходить в вашу систему, тоже будут изучать этот стек, огромный объем информации.

Kubernetes — не единственный важный кубик, есть много других важных кубиков вокруг, без которых система работать не будет. То есть, нужно очень сильно беспокоиться об отказоустойчивости.

05:44 12а слайд с кошкой

Из-за этого Kubernetes-у минус. Система сложная, нужно много о чем заботиться.

05:55 12б слайд про плюсы

Но есть и плюсы. Если человек изучил Kubernetes в одной компании, в другой у него не станут волосы дыбом из-за системы релизов. С течением времени, когда Kubernetes захватит большее пространство, переход людей и обучение будет более простым. И за это — плюс.

06:14 12в слайд с котом Саймона

06:16 13 слайд, опыт Helm [10]

Мы используем Helm. Это система, котоаря строится поверх Kubernetes, напоминает пакетный менеджер. Вы можете нажать кнопку, сказать, я хочу установить wine? (06:28) в мою систему. Можно устанавливать и в Kubernetes. Оно работает, автоматически скачает, запустит и все будет работать. Он позволяет работать с плагинами, клиент-серверная архитектура. Если вы будете с ним работать, рекомендуем запускать один Tiller на namespace. Это изолирует namespace друг от друга, и поломка одного не приведет к поломке другого.

07:08 13а слайд, Helm с кошкой

На самом деле система очень сложная. Система, которая должна быть абстракцией более высокого уровня и более простой и понятной, на самом деле не делает понятнее нисколечко. За это минус.

07:16 14 слайд, примеры конфигураций

Давайте сравним конфиги. Скорее всего, у вас тоже есть какие-то конфиги, если вы запускаете в продакшене вашу систему. У нас есть своя система, которая называется BOOMer. Я не знаю, почему мы ее так назвали. Она состоит из puppet, chef, ansible, terraform и всего остального, там большой флакон.

07:33, 15 слайд, как работает бумер

Давайте посмотрим, как оно работает. Вот пример реальной конфигурации, которая сейчас работает в продакшене. Что мы здесь видим?

07:33 15а слайд, стрелочки

Во-первых, мы видим где запускать приложение, во-вторых, что надо запускать и, в-третьих, как это надо подготовить к запуску. В одном флаконе уже смешаны концепции.

07:56 15б слайд, продолжение

Если мы посмотрим дальше, из того что мы добавили наследование, чтобы сделать более сложные конфиги, мы должны посмотреть на то, что находится в конфиге common, на который ссылаемся. Плюс, мы добавляем настройку сетей, прав доступа, планирование нагрузки. Все это в одном конфиге, который нам нужен для того, чтобы запустить реальное приложение в продакшене, мы смешиваем кучу концепций в одном месте.

Это очень сложно, это очень неправильно, и в этом огромный плюс Kubernetes, потому что в нем вы просто определяете, что запустить. Настройка сети была выполнена при установке Kubernetes, настройка всего provisioning решается с помощью докера — у вас произошла инкапсуляция, все проблемы каким-то образом разделились, и в данном случае в конфиге есть только ваше приложение, и за это плюс.

08:50 15в слайд, с котом

Давайте посмотрим внимательнее. Здесь у нас есть только одно приложение. Чтобы заработал деплоймент, нужно, чтобы работала еще куча всего. Во-первых, нужно определить сервисы. Каким образом к нам поступают секреты, ConfigMap, доступ к Load Balancer.

09:03, 15г слайд, сервисы.

Не стоит забывать, что у вас есть несколько окружений. Есть Stage/Prod/Dev. Это все вместе составляет не маленький кусочек, который я показал, а огромный набор конфигов, что на самом деле сложно. За это минус.

09:19, 15д слайд, минус

09:21, 16 слайд, шаблон helm

Helm-шаблон для сравнения. Он полностью повторяет шаблоны Kubernetes, если какой-то файл в Kubernetes с определением деплоймента, тоже самое будет в Helm. Вместо конкретных значений для окружения у вас есть шаблоны, которые подставляются из values

09:41 16б слайд, values

У вас есть отдельно шаблон, отдельно значения, которые должны подставиться в этот шаблон.

09:47, 16в слайд, доп. параметры

Конечно, нужно дополнительно определить различную инфраструктуру самого Helm, притом, что у вас в Kubernetes масса конфигурационных файлов, которые нужно перетащить в Helm. Это все очень непросто, за что минус.

09:58, 16г слайд, кот

Система, которая должна упрощать, на самом деле усложняет. Для меня это явный минус. Либо нужно надстраивать что-то еще, либо не использовать

10:11 17 слайд, погружаемся глубже.

Давайте пойдем поглубже, мы недостаточно глубоко.

10:14, 18 слайд, кластера

Во-первых, как мы работаем с кластерами. Я прочитал статью Гугла Work Omega и Kubernetes, в ней очень сильно защищают концепцию, что нужно иметь один большой кластер. Я тоже был за эту идею, в конце концов мы от нее ушли. В результате наших споров используем четыре разных кластера.

Первый кластер е2е, для тестирования самого Kubernetes и тестирования скриптов, разворачивающих окружение, плагины и так далее. Второй, конечно же, prod и stage. Это стандартные концепции. В-третьих, это admin, в котором сгрузилось все остальное, в частности, у нас там CI, и, похоже, из-за него этот кластер будет самым большим всегда.

Тестирований очень много: по коммиту, по merge, все делают кучу коммитов, поэтому кластеры просто громадные.

11:06, 19 слайд, опыт CoreOS

Мы пытались посмотреть на CoreOS, но не стали ее использовать. У них внутри TF или CloudFormation, и то и другое очень плохо позволяет понимать, что находится внутри state. Из-за этого возникают проблемы при обновлении. Когда вы хотите обновить настройки вашего Kubernetes, к примеру, его версию, можно столкнуться с тем, что обновление происходит не таким образом, не в той последовательности. Это большая проблема стабильности.

11:48, 19а слайд, кошка

Это минус.

11:53. 20 слайд, внутренний репозиторий образов

Во-вторых, когда вы используете Kubernetes, нужно откуда-то скачивать образы. Это может быть внутренний источник, репозиторий, или внешний. Если внутренний, есть свои проблемы. Я рекомендую использовать Docker Distribution, потому что она стабильная, ее сделал Docker. Но цена поддержки все равно высокая. Чтобы она работала, нужно сделать ее отказоустойчивой, потому что это единственное место, откуда ваши приложения получают данные для работы.

Представьте, что в самый ответственный момент, когда вы нашли баг на продакшене, у вас репозиторий упал — приложение обновить вы не сможете. Вы должны сделать его отказоустойчивым, причем от всех возможных проблем, которые только могут быть.

Во-вторых, если масса команд, у каждой свой образ, их накапливается очень много и очень быстро. Можно убить свой Docker Distribution. Нужно делать чистку, удалять образы, выносить информацию для пользователей, когда и что вы будете чистить.

В-третьих, при больших образах, скажем, если у вас есть монолит, размер образа будет очень большим. Представьте, что нужно зарелизить на 30 нод. 2 гигабайта на 30 нод — посчитайте, какой поток, как быстро он скачается на все ноды. Хотелось бы, чтобы нажал кнопку и тут же зарелизилось. Но, нет, нужно сначала дождаться, пока закачается. Надо как-то ускорять эту закачку, а все это работает с одной точки.

13:25, 21 слайд, внешний репозиторий образов

При внешних репозиториях есть те же проблемы с garbage collector, но чаще всего это делается автоматически. Мы используем Quay. В случае с внешними репозиториями — это сторонние сервисы, в которых большинство образов публичные. Чтобы не было публичных образов, нужно обеспечивать доступ. Нужны секреты, права доступа к образам, все это специально настраивать. Конечно, этом можно автоматизировать, но в случае локального запуска куба на своей системе вам все равно его придется настраивать.

14:11, 22 слайд, kops

Для установки Kubernetes мы используем kops. Это очень хорошая система, мы ранние пользователи, когда они еще в блоге не писали. Она не до конца поддерживает CoreOS, хорошо работает с Debian, умеет автоматически конфигурировать мастерноды Kubernetes , работает с аддонами, есть способность делать нулевое время простоя во время обновления Kubernetes.

Все эти возможности из коробки, за что большой и жирный плюс. Отличная система

14:47, 22а слайд с котом

14:50, 23 слайд, сети

По ссылке [23] можете найти много вариантов для настройки сети в Kubernetes. Их реально много, у всех свои достоинства и недостатки. Kops поддерживает только часть из этих вариантов. Можно, конечно, донастроить, чтобы работал через CNI, но лучше использовать самые популярные и стандартные. Они тестируются сообществом, и, скорее всего, стабильны.

15:22, 24 слайд, Calico.

Мы решили использовать Calico. Он заработал хорошо с нуля, без большого количества проблем, использует BGP, быстрее инкапсуляции, поддерживает IP-in-IP, позволяет работать с мультиклаудами, для нас это большой плюс.

Хорошая интеграция с Kubernetes, с помощью меток разграничивает трафик. За это — плюс.

15:51, 24а слайд, кот Саймона

Я не ожидал, что Calico дойдет до состояния, когда включил и все работает без проблем.

16:01, 25 слайд, НА

High Availability, как я говорил, мы делаем через kops, можно использовать 5-7-9 нод, мы используем три. Сидим на etcd v2, из-за бага не обновлялись на v3. Теоретически, это позволит ускорить какие-то процессы. Я не знаю, сомневаюсь.

16:23, 26 слайд, восстановление после сбоев

Восстановление после сбоев. Хитрый момент, у нас есть специальный кластер для экспериментов со скриптами, автоматическая накатка через CI. Мы считаем, что у нас есть защита от совершенно неправильных действий, но для каких-то специальных и сложных релизов мы на всякий случаем делаем снапшоты всех дисков, мы не делаем бэкапов каждый день.

16:52, 27 слайд, аутентификация
16:54, 28 слайд, доступ до Kubernetes

Авторизация — вечный вопрос. Мы в Kubernetes используем RBAC, доступ, основанный на ролях. Он намного лучше ABAC, если вы его настраивали, то понимаете, о чем я. Посмотрите на конфиги — удивитесь.

Мы используем Dex, провайдер OpenID, который из какого-то источника данных выкачивает всю информацию. А для того, чтобы логиниться в Kubernetes, есть два пути. Нужно как-то прописать в .kube/config куда идти, и что он может делать. Нужно этот конфиг как-то получить. Либо пользователь идет в UI, где он логинится, получает конфиги, копипастит их в /config и работает. Это не очень удобно. Мы постепенно перешли к тому, что человек заходит в консоль, нажимает на кнопку, логинится, у него автоматически генерируются конфиги и складываются в нужное место. Так гораздо удобнее, мы решили действовать таким образом.

18:08, 29 слайд, управление пользователями.

В качестве источника данных мы используем Active Directory. Kubernetes позволяет через всю структуру авторизации протащить информацию о группе, которая транслируется в namespace и роли. Таким образом сразу разграничиваем, куда человек может заходить, куда не имеет права заходить, и что он может релизить.

18:43, 30 слайд, доступ к AWS.

Чаще всего людям нужен доступ к AWS. Если у вас нет Kubernetes, есть машина с запущенным приложением. Казалось бы, все что нужно — получать логи, смотри их и все. Это удобно, когда человек может зайти на свою машину и посмотреть, как работает приложение. С точки зрения Kubernetes все работает в контейнерах. Есть команда — залезть в приложение и посмотреть, что там происходит. Поэтому на AWS инстансы нет необходимости ходить. Мы запретили доступ для всех, кроме инфракоманды.

Более того, мы запретили долгоиграющие админские ключи, вход через роли. Если есть возможность использовать роль админа — я админ. Плюс мы добавили ротацию ключей. Это удобно конфигурировать через команду awsudo, это проект на гитхабе, очень рекомендую, позволяет работать, как с sudo командой.

19:51, 30а слайд, кот саймона

19:55, 31 слайд, управление ресурсами

19:57, 32 слайд, применение квот

Квоты. Очень хорошая штука в Kubernetes, работает прямо из коробки. Вы ограничиваете какие-то namespace, скажем, по количеству объектов, памяти или CPU, которое можете потреблять. Я считаю, что это важно и полезно всем. Мы пока не дошли до памяти и CPU, используем только по количеству объектов, но это все добавим.

20:24, 32а слайд, кот саймона

Большой и жирный плюс, позволяет сделать много хитрых вещей.
20:29, 33 слайд, автоматическое масштабирование

Масштабирование нельзя смешивать внутри Kubernetes и снаружи Kubernetes. Внутри Kubernetes делает масштабирование сам. Он может увеличивать pod-ы автоматически, когда идет большая нагрузка. Я говорю про масштабирование самих инстансов внутри Kubernetes. Это можно делать с помощью AWS Autoscaler, проект на гитхабе.

Когда вы добавляете новый pod, он не может стартовать, потому что ему не хватает ресурсов на всех инстансах, AWS Autoscaler автоматически может добавить ноды. Позволяет работать на Spot-инстансах, мы пока это не добавляли, но будем, позволяет сильно экономить.

21:14, 33а слайд, кот

21:16, 34 слайд, телеметрия

Когда у вас очень много пользователей, и приложений у пользователей, нужно как-то за ними следить. Обычно это телеметрия, логи, какие-то красивые графики.

21:34, 35 слайд, фреймворки для телеметрии

У нас по историческим причинам был Sensu, он не очень подошел для Kubernetes. Нужен было более метрикоориентированный проект. Мы посмотрели на весь TICK стек, особенно InfluxDB. Хороший UI, SQL-подобный язык, но не хватило немножко фич. Мы перешли на Prometheus.

Он хорош. Хороший язык запросов, хорошие алерты, и все из коробки.

22:04. 36 слайд, Cernan

Чтобы посылать телеметрию, мы использовали Cernan. Это наш собственный проект, написанный на Rust. Это единственный проект на Rust, который уже год работает в нашем продакшене. У него есть несколько концепций: есть концепция источника данных, вы конфигурируете несколько источников. Вы конфигурируете куда данные будете сливать. У нас есть конфигурация фильтров, то есть перетекающие данные можно каким-то образом перерабатывать. Вы можете преобразовывать логи в метрики, метрики в логи, все что хотите.

При том, что у вас несколько входов, несколько выводов, и вы показываете что куда идет, там что-то вроде большой системы графов, получается довольно удобно.

22:53, 37 слайд, метрики

Мы сейчас плавно переходим с текущего стека Statsd/Cernan/Wavefront на Kubernetes. Теоретически, Prometheus хочет сам забирать данные из приложений, поэтому во все приложения нужно добавлять endpoint, из которого он будет забирать метрики. Cernan является передаточным звеном, дожен работать везде. Тут две возможности: запускать на каждом инстансе Kubernetes, можно с помощью Sidecar-концепции, когда в вашем поле данных работает еще один контейнер, который посылает данные. Мы делаем и так, и так.

23:48, 38 слайд, логирование.

У нас прямо сейчас все логи шлются в stdout/stderr. Все приложения рассчитаны на это, поэтому одно из критических требований, чтобы мы не уходили от этой системы. Cernan посылает данные в ElasticSearch, события всей системы Kubernetes посылают туда же с помощью Heapster. Это очень хорошая система, рекомендую.

После этого все логи вы можете посмотреть в одном месте, к примеру, в консоли. Мы используем Kibana. Есть замечательный продукт Stern, как раз для логов. Он позволяет смотреть, раскрашивает в разные цвета разные поды, умеет видеть, когда один под умер, а другой рестартовал. Автоматически подхватывает все логи. Идеальный проект, его очень рекомендую, это жирный плюс Kubernetes, здесь все хорошо.

24:50, 38а слайд, кот

24:53, 39 слайд, секреты.

Секреты. Мы используем S3 и KMS. Думаем о переходе на Vault или секреты в самом Kubernetes. Они были в 1.7в состоянии альфы, но что-то делать с этим надо.

25:07, 40 слайд, разработка

Мы добрались до интересного. Разработка в Kubernetes вообще мало рассматривается. В основном говорится: «Kubernetes — идеальная система, в ней все хорошо, давайте, переходите». Слайд отлично иллюстрирует: разработчикам говорят: «Ну вот твой сыр, давай уже, ешь его быстрее».

Но на самом деле бесплатный сыр только в мышеловке, а для разработчиков в Kubernetes ад.

25:33, 41 слайд, ад

Не с той точки зрения, что все плохо, а с той, что надо немножко по-другому взглянуть на вещи. Я разработку в Kubernetes сравниваю с функциональным программированием: пока ты его не коснулся, ты думаешь в своем императивном стиле, все хорошо. Для того, чтобы разрабатывать в функциональщине, надо немножко повернуть голову другим боком — здесь тоже самое.

Разрабатывать можно, можно хорошо, но нужно смотреть на это иначе. Во-первых, разобраться с концепцией Docker-way. Это не то, чтобы сложно, но до конца ее понять довольно проблематично. Большинство разработчиков привыкло, что они заходят на свою локальную, удаленную или виртуальную машину, говорят: «Саша, давай я тут кое-что подправлю, подшаманю».

Ты говоришь ему, что в Kubernetes так не будет потому что у тебя read only инфраструра. Когда ты хочешь обновить приложение, пожалуйста, сделай новый образ, который будет работать, а старый, пожалуйста, не трогай, он просто умрет. Я лично работал над внедрением Kubernetes в разные команды и вижу ужас в глазах людей, когда они понимают, что все старые привычки придется полностью отбросить, придумать новые, новую систему какую-то, а это очень сложно.

Плюс придется много делать выборов: скажем, когда делаешь какие-то изменения, если разработка локальная, нужно как-то коммитать в репозиторий, затем репозиторий по pipeline прогоняет тесты, а потом говорит «ой, тут опечатка в одном слове», нужно все делать локально. Монтировать каким-то образом папку, заходить туда, обновлять систему, хотя бы компилировать. Если тесты запускать локально неудобно, то может коммитать в CI хотя бы, чтобы проверять какие-то локальные действия, а затем уже отправлять их в CI на проверку. Эти выборы достаточно сложные.

Особенно сложно, когда у вас развесистое приложение, состоящее из ста сервисов, а чтобы работал один из них, нужно обеспечить работу всех остальных рядышком. Нужно либо эмулировать окружение, либо как-то запускать локально. Весь этот выбор нетривиальный, разработчику об этом нужно сильно думать. Из-за этого возникает негативное отношение к Kubernetes. Он, конечно, хорош, но он плох, потому что нужно много думать и изменять свои привычки.

28:00 42 слайд, 3 жирных кошки

Поэтому здесь три жирных кошки перебежали через дорогу.

28:01, 43 слайд, опыт: инструменты разработчика

Когда мы смотрели на Kubernetes, старались посмотреть, может есть какие-то удобные системы для разработки. В частности, есть такая вещь, как Deis, наверняка вы все про нее слышали. Она очень проста в использовании, и мы провели на самом деле все основные проекты. Они легко переходят, надеюсь, но проблема в том, что более сложные проекты туда не берите.

Как я уже рассказал, перешли на Helm Charts. Но единственная проблема, которую мы сейчас видим — нужно очень много хорошей документации. Нужны какие-то How-to, какие-то FAQ, чтобы человеку быстро стартовать, скопировать текущий конфиги вставить свой, поменять названия для того, чтобы все было правильно. Это тоже важно понимать заранее и делать. Общий набор инструментов для разработки у меня здесь перечислены, всего этого не коснусь, кроме миникуба.

29:05, 44 слайд миникуб

Minikube — очень хорошая система, в том смысле, что хорошо, что она есть, но плохо, что в таком виде. Она позволяет запускать Kubernetes локально, позволяет видеть все на вашем лэптопе, не надо никуда ходить по SSH и так далее.

Я работаю на MacOS, у меня Mac, соответственно, чтобы запускать локальный апп мне нужно запускать локально докер. Это сделать никак нельзя. В итоге, нужно либо запускать virtualbox, либо xhyve. Обе вещи, фактически, эмуляции поверх моей операционной системы. Мы используем hhyve, но рекомендуем использовать VirtualBox, поскольку очень много багов, их приходится обходить.

Но сама идея, что есть виртуализации, а внутри виртуализации запускается еще один уровень абстракции для виртуализации какая-то нелепая, бредовая. В целом, хорошо что он как-то работает, но лучше бы вы его еще доделали.

30:07, 45 слайд, кот
30:09, 46 слайд, CI

CI не относится напрямую к Kubernetes, но это очень важная система, особенно, если у вас есть Kubernetes, если ее интегрировать, можно получать очень хорошие результаты. Мы использовали Concourse для CI, очень богатая функциональность, можно строить страшные графы что, откуда, как запускается, от чего зависит. Но разработчики очень странно относятся к своему продукту. Скажем, при переходе от одной версии на другую они поломали обратную совместимость и большинство плагинов не переписали. Более того, документацию не дописали, и когда мы попытались что-то сделать, вообще ничего не заработало.

Документации мало вообще во всех CI, приходится читать код, и, в общем, мы отказались. Перешли на Drone.io — он маленький, очень легкий, юркий, функциональности намного меньше, но чаще всего ее хватает. Да, большие и увесистые графы зависимости — было бы удобно, но и на маленьких тоже можно работать. Тоже мало документации, читаем код, но это ок.

Каждая стадия pipeline работает в своем докер-контейнере, это позволяет сильно упростить переход на Kubernetes. Если есть приложение, которое работает на реальной машине, для того, чтобы добавить в CI, используете докер-контейнер, а после него перейти в Kubernetes проще простого.

У нас настроен автоматический релиз в admin/stage кластера, в продакшен-кластер пока боимся добавлять настройку. Плюс есть система плагинов.

31:51, 47 слайд, конфиг

Это пример простого Drone-конфига. Взято из готовой рабочей системы, в данном случае есть пять шагов в pipeline, каждый шаг что-то делает: собирает, тестирует и так далее. При том наборе фич, что есть в Drone, я считаю, что это хорошая штука,
32:10, 47а слайд, кот
32:14, 48 слайд, оценка Google

Мы много спорили о том, сколько иметь кластеров: один или несколько. Когда мы пришли к идее с несколькими кластерами, начали дальше работать в этом направлении, создали какие-то скрипты, понаставили кучу других кубиков к нашему Kubernetes. После этого пришли к Google и попросили консультации, все ли сделали так, может нужно что-то исправить.

Google согласился, что идея одного кластера неприменима в Kubernetes. Есть много недоделок, в частности, работа с геолокациями. Получается, что идея верна, но говорить о ней пока рано. Возможно, попозже. Пока может помочь Service Mesh.

33:12, 49 слайд, Geodesic

В целом, если хотите посмотреть как работает наша система, обратите внимание на Geodesic. Это продукт, похожий на тот, что делаем мы. Он с открытыми исходниками, очень похожий выбор концепции дизайна, посмотрите. Мы думаем о том, чтобы объединиться и, возможно, использовать их.

33:34, 50 слайд, текущие проблемы.

Да, есть проблемы. Есть боль с локальными именами, с сертификатами. Есть проблема с загрузками больших образов и их работой, возможно, связанная с файловой системой, мы туда еще не копали. Есть уже три разных способа устанавливать расширения Kubernetes. Мы меньше года работаем над этим проектом, и у нас уже три разных способа, годовые кольца растут.

34:06, 51 слайд, минусы 1.

Давайте подобьем все минусы. Итак, я считаю, что один из главных минусов — большой объем информации на изучение, причем не только новых технологий, но и новых концепций и привычек. Это как новый язык выучить: в принципе, не сложно, но сложно немного повернуть голову всем пользователям. Если раньше не работали с подобными концепциями — перейти на Kubernetes тяжело.

Kubernetes будет лишь небольшой частью вашей системы. Все думают, что установят Kubernetes, и все сразу заработает. Нет, это маленький кубик, и таких кубиков будет много.

Некоторые приложения в принципе сложно запускать на Kubernetes и лучше не запускать. Также очень тяжелые и большие конфигурационные файлы, и в концепциях поверх Kubernetes еще сложнее. Все текущие решения — сырые.

Все эти минусы, конечно, отвратительные.

35:07, 52 слайд, минусы 2

Компромиссы и сложный переход создают негативный образ Kubernetes, и как с этим бороться — я не знаю. Мы не смогли сильно побороть, есть люди, которые ненавидят все это движение, не хотят и не понимают его плюсы.

Для того, чтобы запустить minicube, вашу систему, чтобы все работало, придется сильно постараться. Как вы видите, минусов немало, и те, кто не хочет работать с Kubernetes имеют свои причины. Если не хотите слышать про плюсы — закройте глаза и уши, потому что дальше пойдут они.

35:55, 53 слайд, плюсы 1

Первый плюс — с течением времени придется все меньше учить новичков. Часто бывает так, что, придя в систему, начинает вырывать волосы, потому что первые 1-2 месяца он пытается разобраться, как это все зарелизить, если система большая и живет долго, наросло много годовых колец. В Kubernetes все будет проще.

Во-вторых, Kubernetes не сам делает, но позволяет сделать короткий цикл релиза. Коммит создал CI, CI создал образ, автоматически подкачался, ты нажал кнопку и все ушло в продакшен. Это сильно сокращает время релиза.

Следующее — разделение кода. Наша система, и большинство ваших систем, в одном месте собирают конфиги разных уровней, то есть у вас инфраструктурный код, бизнес код, все логики смешаны в одном месте. В Kubernetes такого не будет из коробки, выбор правильной концепции помогает этого избежать заранее.

Большое и очень активное сообщество, а значит, большое количество изменений. Большинство того, о чем я упоминал, за последние два года стали настолько стабильными, что их можно выпускать в продакшен. Возможно, часть из них появилась пораньше, но но была не очень стабильной.

Я считаю большим плюсом, что в одном месте можно видеть как логи приложения, так и логи работы Kubernetes с вашим приложением, что неоценимо. И нет доступа на ноды. Когда мы убрали у пользователей доступы на ноды, это сразу срезало большой класс проблем.

37:22, 54 слайд, плюсы 2

Вторая часть плюсов немного более концептуальна. Большая часть сообщества Kubernetes видят технологическую часть. Но мы увидели концептуальную менеджмент-часть После того, как вы перейдете на Kubernetes, и если это все правильно настроить, инфракоманда, бэкенд, я не знаю, как у вас это называется правильно, больше не нужна для того, чтобы релизить приложения.

Пользователь хочет зарелизить приложение, он не придет с просьбой об этом, а просто запустит новый pod, вот команда для этого. инфракоманда не нужна для, чтобы расследовать проблемы. Достаточно посмотреть логи, у нас набор конструкций не такой большой, есть список, по которому найти проблему очень легко. Да, иногда нужна поддержка, если проблема в Kubernetes, в инстансах, но чаще всего проблема с приложениями.

Мы добавили Error Budget. Это концепция: у каждой команды есть статистика, сколько происходит проблем в продакшене. Если происходит слишком много, им режут релизы, пока не пройдет какое-то время. Это хорошо тем, что команда будет серьезно следить, чтобы их релизы были очень стабильными. Нужна новая функциональность — пожалуйста, релизьте. Хотите релизить в два часа ночи — пожалуйста. Если за все релизы одни «девятки» — делайте, что хотите, все стабильно, можете делать что угодно. Однако, если ситуация гораздо хуже, скорее всего, мы не разрешим релизить, кроме фиксов.

Это удобная вещь как для стабильности системы, так и для настроя внутри команды. Мы перестаем быть «Полицией Нравов», не давая релизить поздно вечером. Делайте, что хотите, пока у вас хороший бюджет ошибок. Это сильно сокращает напряжение внутри компании.

39:56, 55 слайд, ссылки
39:59, 56 слайд, ссылки 2
В конце у меня два слайда с ссылками, вы потом сможете их скачать и все посмотреть.

Вопросы и ответы

Drone уже научился работать с Kubernetes API или вы, когда запускаете какие-то сборки в Drone, используете Docker API для запуска тех, кто будет собирать приложение? Есть серверы, есть worker-ы. Понятно, что это без проблем можно запустить в Kubernetes. Вопрос, как worker-ы запускают среду для выполнения каких-то execution-ов? Раньше, насколько мне известно, это запускалось в Docker. Вы говорите, что сейчас это работает в Kubernetes.

Мы не запускаем Drone в Kubernetes. Это отдельная система, я, может, не так выразился, прошу прощения.

Какой процент продакшена у вас сейчас переехал на Kubernetes?

Переход на Kubernetes возможен только после того, как он станет готов к продакшену. У нас первые полгода заняла только подготовка к продакшену. Следующие пару-тройку месяцев мы перетаскиваем сервисы. Я бы сказал, что нас пока это около 5-10 %. Самая большая часть бизнес-логики у нас в большом монолите, которые нельзя пока растащить на сервисы. То есть все маленькие кусочки, которые можно, мы перетаскиваем, а вот этот большой монолит требует очень хорошей подготовки для перетаскивания. Сейчас это в процессе, мы надеемся что в ближайшие два месяца мы его перетащим.

Монолит — это то, что у вас было указано в минусах? Это какое-то stateful-приложение, которое в Docker или Kubernetes не запихивается? Почему вы не можете это сделать?

Он запихивается, он хранит у нас все в базах данных, то есть он частично stateful, я бы так сказал. Сама работа с Docker, само тестирование — все эти годовые кольца… Нужно же всех разработчиков заставить перейти после этого в Docker, ты не можешь просто перетащить все в Kubernetes. Нужно подготовить среду, тестирование, много чего для того, чтобы это все перетащить. Это не проблема Kubernetes, это проблема бизнес-логики, большого объема и годовых колец.

- Расскажите про последнее падение в продакшене Kubernetes, о котором вы говорили.

Падений Kubernetes, как таковых, пока не было. Все вещи, которые я сейчас навскидку вспоминаю, — это большинство обвязок. Скажем, у нас была проблема с Docker distribution. Просто заканчивается место, и он не может ничего больше сделать. Были проблемы с логами. Все эти мелочи не про сам Kubernetes, не про инфраструктуру, а про работу с этим Kubernetes. Поэтому проблемы Kubernetes как такового у нас пока не было.

Иван, вопрос такой: как вы смотрите на микросервисы и Kubernetes. Речь заходит о том, что микросервисов больше ста, имеет ли смысл использовать для этого Kubernetes?

Kubernetes идеально подходит для микросервисов.

Второй вопрос инфраструктурный: пытались ли вы организовать инфраструктуру вокруг CentOS или Debian 9? Почему именно Debian 8?

Я думаю, по историческим причинам. У нас уже начали образовываться исторические кольца даже внутри проекта. С другими системами мы попробовали, просто на нем быстрее всего зашло, и мы продолжаем работать. Может быть, мы перейдем когда-нибудь.

Вопрос для разработчиков. Для локального тестирования вы используете виртуалки, не пробовали использовать для MacOS Wine?

Нет. VirtualBox хорош, xhive чуть похуже, а чем Wine поможет?

На чем запускаете Kubernetes: AWS, Google Cloud, еще на чем-то?

У нас цель сделать мультиклаудное решение. Сейчас работает только на AWS в двух зонах доступности. В дальнейшем мы хотим сделать на нескольких зонах плюс Google Cloud, чтобы оно было объединенное.

С помощью чего поднимайте инфраструктуру на AWS для использования ее потом на Kubernetes?

Kops/terraform

Сколько у вас человек в инфракоманде?

Над Kubernetes работают три человека. Два человека — первые полгода, и человек добавился в следующие полгода. У нас примерно около года этому проекту. В инфракоманде всего у нас 17 человек, но остальная часть не работает над Kubernetes. Возможно, помощь в запуске, но не сам Kubernetes.

У вас самый большой кластер для тестирования Kubernetes. А в чем заключается процесс тестирования?

Не тестирование Kubernetes. Я, может быть, неправильно выразился. У нас этот кластер используется для всего. Есть кластер i2i, который для тестирования самого Kubernetes, и на нем обкатываются все релиз-процедуры, изменения Kubernetes, конфигурации и так далее. Есть второй кластер, админ он у нас называется, он используется для CI всех остальных девелоперов. То есть все коммиты, которые идут в Github автоматически тестируются через CI как раз внутри этого кластера. Речь идет о бизнес-логике.

А как непосредственно сам Kubernetes тестируется?

Вот это как раз i2i-кластер. Там минимальное количество нод, 3-5, сколько нужно для тестов.
А шаги какие?

Там тоже есть автоматизация, есть отдельный CI pipeline, в котором после накатывания изменений прокатываются тесты: поднять кластер, запустить кластер и тому подобное.
